@article{Kamei2013,
	abstract = {Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone ({\&} risky{\&}) software changes instead of files or packages. We refer to this type of quality assurance activity as {\&} Just-In-Time Quality Assurance,{\&} because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that {\&} Just-In-Time Quality Assurance{\&} may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software. {\textcopyright} 1976-2012 IEEE.},
	author = {Kamei, Yasutaka and Shihab, Emad and Adams, Bram and Hassan, Ahmed E. and Mockus, Audris and Sinha, Anand and Ubayashi, Naoyasu},
	doi = {10.1109/TSE.2012.70},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/A Large-Scale Empirical Study of Just-in-Time Quality Assurance.pdf:pdf},
	issn = {00985589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {Maintenance,defect prediction,just-in-time prediction,mining software repositories,software metrics},
	mendeley-groups = {Primary Approaches,Approach},
	number = {6},
	pages = {757--773},
	title = {{A large-scale empirical study of just-in-time quality assurance}},
	volume = {39},
	year = {2013}
}
@article{Yang2015,
	abstract = {Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22{\%} more bugs than Kamei et al's approach (51.04{\%} versus 18.82{\%} on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects.},
	author = {Yang, Xinli and Lo, David and Xia, Xin and Zhang, Yun and Sun, Jianling},
	doi = {10.1109/QRS.2015.14},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Deep Learning for Just-In-Time Defect Prediction.pdf:pdf},
	isbn = {9781467379892},
	journal = {Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015},
	keywords = {Cost Effectiveness,Deep Belief Network,Deep Learning,Just-In-Time Defect Prediction},
	mendeley-groups = {Approach,Primary Approaches},
	number = {1},
	pages = {17--26},
	title = {{Deep Learning for Just-in-Time Defect Prediction}},
	year = {2015}
}
@article{Koyuncu2019,
	abstract = {Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and-validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).},
	annote = {Gilt das auch als "Commit-Validation"?},
	archivePrefix = {arXiv},
	arxivId = {1907.05620},
	author = {Koyuncu, Anil and Liu, Kui and Bissyand{\'{e}}, Tegawend{\'{e}} F. and Kim, Dongsun and Monperrus, Martin and Klein, Jacques and {Le Traon}, Yves},
	doi = {10.1145/3338906.3338935},
	eprint = {1907.05620},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/iFixR - Bug Report driven Program Repair.pdf:pdf},
	isbn = {9781450355728},
	keywords = {2,acm reference format,anil koyuncu 1,automatic patch generation,bissyand{\'{e}} 1,dongsun kim 1,fault localization,information retrieval,kui liu 1,mar-,tegawend{\'{e}} f},
	mendeley-groups = {Approach},
	pages = {314--325},
	title = {{iFixR: bug report driven program repair}},
	year = {2019}
}
@article{Nayrolles2018Doctor,
	abstract = {Software maintenance activities such as debugging and feature enhancement are known to be challenging and costly, which explains an ever growing line of research in software maintenance areas including mining software repository, default prevention, clone detection, and bug reproduction. The main goal is to improve the productivity of software developers as they undertake maintenance tasks. Existing tools, however, operate in an offline fashion, i.e., after the changes to the systems have been made. Studies have shown that software developers tend to be reluctant to use these tools as part of a continuous development process. This is because they require installation and training, hindering their integration with developers' workflow, which in turn limits their adoption. In this thesis, we propose novel approaches to support software developers at commit-time. As part of the developer's workflow, a commit marks the end of a given task. We show how commits can be used to catch unwanted modifications to the system, and prevent the introduction of clones and bugs, before these modifications reach the central code repository. We also propose a bug reproduction technique that is based on model checking and crash traces. Furthermore, we propose a new way for classifying bugs based on the location of fixes that can serve as the basis for future research in this field of study. The techniques proposed in this thesis have been tested on over 400 open and closed (industrial) systems, resulting in high levels of precision and recall. They are also scalable and non-intrusive.},
	author = {Nayrolles, Mathieu},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Software Maintenance at Commit Time.pdf:pdf},
	mendeley-groups = {Related Information},
	number = {August},
	title = {{Software Maintenance At Commit-Time}},
	year = {2018}
}
@article{Bader2019,
	abstract = {Static analyzers help find bugs early by warning about recurring bug categories. While fixing these bugs still remains a mostly manual task in practice, we observe that fixes for a specific bug category often are repetitive. This paper addresses the problem of automatically fixing instances of common bugs by learning from past fixes. We present Getafix, an approach that produces human-like fixes while being fast enough to suggest fixes in time proportional to the amount of time needed to obtain static analysis results in the first place. Getafix is based on a novel hierarchical clustering algorithm that summarizes fix patterns into a hierarchy ranging from general to specific patterns. Instead of an expensive exploration of a potentially large space of candidate fixes, Getafix uses a simple yet effective ranking technique that uses the context of a code change to select the most appropriate fix for a given bug. Our evaluation applies Getafix to 1,268 bug fixes for six bug categories reported by popular static analyzers for Java, including null dereferences, incorrect API calls, and misuses of particular language constructs. The approach predicts exactly the human-written fix as the top-most suggestion between 12{\%} and 91{\%} of the time, depending on the bug category. The top-5 suggestions contain fixes for 526 of the 1,268 bugs. Moreover, we report on deploying the approach within Facebook, where it contributes to the reliability of software used by billions of people. To the best of our knowledge, Getafix is the first industrially-deployed automated bug-fixing tool that learns fix patterns from past, human-written fixes to produce human-like fixes.},
	annote = {Potentiall auch eine Art der "Commit Validation", allerdings mit dem Trigger "Code with static analysis warning".},
	archivePrefix = {arXiv},
	arxivId = {1902.06111},
	author = {Bader, Johannes and Scott, Andrew and Pradel, Michael and Chandra, Satish},
	eprint = {1902.06111},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Getafix - Learning to Fix Bugs Automatically.pdf:pdf},
	keywords = {approach,secondaryapproach},
	mendeley-groups = {Approach},
	mendeley-tags = {approach,secondaryapproach},
	title = {{Getafix: Learning to Fix Bugs Automatically}},
	url = {http://arxiv.org/abs/1902.06111},
	year = {2019}
}
@article{Shaw2002,
	abstract = {Physics, biology, and medicine have well-refined public explanations of their research processes. Even in simplified form, these provide guidance about what counts as "good research" both inside and outside the field. Software engineering has not yet explicitly identified and explained either our research processes or the ways we recognize excellent work. Science and engineering research fields can be characterized in terms of the kinds of questions they find worth investigating, the research methods they adopt, and the criteria by which they evaluate their results. I will present such a characterization for software engineering, showing the diversity of research strategies and the way they shift as ideas mature. Understanding these strategies should help software engineers design research plans and report the results clearly; it should also help explain the character of software engineering research to computer science at large and to other scientists. {\textcopyright} 2002 Springer-Verlag.},
	author = {Shaw, Mary},
	doi = {10.1007/s10009-002-0083-4},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/What Makes Good Research in Software Engineering.pdf:pdf},
	issn = {14332779},
	journal = {International Journal on Software Tools for Technology Transfer},
	mendeley-groups = {On Studies},
	number = {1},
	pages = {1--7},
	title = {{What makes good research in software engineering?}},
	volume = {4},
	year = {2002}
}
@article{Kitchenham2004,
	abstract = {The objective of this report is to propose a guideline for systematic reviews appropriate for software engineering researchers, including PhD students. A systematic review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guideline presented in this report was derived from three existing guidelines used by medical researchers. The guideline has been adapted to reflect the specific problems of software engineering research. The guideline covers three phases of a systematic review: planning the review, conducting the review and reporting the review. It is at a relatively high level. It does not consider the impact of question type on the review procedures, nor does it specify in detail mechanisms needed to undertake meta-analysis.},
	author = {Kitchenham, Barbara},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Procedures for Performing Systematic Reviews.pdf:pdf},
	mendeley-groups = {On Studies},
	title = {{Procedures for Performing Systematic Reviews}},
	year = {2004}
}
@article{Hata2018,
	abstract = {Bug fixing is generally a manually-intensive task. However, recent work has proposed the idea of automated program repair, which aims to repair (at least a subset of) bugs in different ways such as code mutation, etc. Following in the same line of work as automated bug repair, in this paper we aim to leverage past fixes to propose fixes of current/future bugs. Specifically, we propose Ratchet, a corrective patch generation system using neural machine translation. By learning corresponding pre-correction and post-correction code in past fixes with a neural sequence-to-sequence model, Ratchet is able to generate a fix code for a given bug-prone code query. We perform an empirical study with five open source projects, namely Ambari, Camel, Hadoop, Jetty and Wicket, to evaluate the effectiveness of Ratchet. Our findings show that Ratchet can generate syntactically valid statements 98.7{\%} of the time, and achieve an F1-measure between 0.29 - 0.83 with respect to the actual fixes adopted in the code base. In addition, we perform a qualitative validation using 20 participants to see whether the generated statements can be helpful in correcting bugs. Our survey showed that Ratchet's output was considered to be helpful in fixing the bugs on many occasions, even if fix was not 100{\%} correct.},
	archivePrefix = {arXiv},
	arxivId = {1812.07170},
	author = {Hata, Hideaki and Shihab, Emad and Neubig, Graham},
	eprint = {1812.07170},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Learning to Generate Corrective Patches Using Neural Machine Translation.pdf:pdf},
	mendeley-groups = {Approach},
	number = {8},
	pages = {1--20},
	title = {{Learning to Generate Corrective Patches using Neural Machine Translation}},
	url = {http://arxiv.org/abs/1812.07170},
	volume = {14},
	year = {2018}
}
@article{Yang2016,
	abstract = {Unsupervised models do not require the defect data to build the prediction models and hence incur a low building cost and gain a wide application range. Consequently, it would be more desirable for practitioners to apply unsupervised models in effort-Aware just-in-Time (JIT) defect prediction if they can predict defect-inducing changes well. However, little is currently known on their prediction effectiveness in this context. We aim to investigate the predictive power of simple unsupervised models in effort-Aware JIT defect prediction, especially compared with the state-of-The-Art su-pervised models in the recent literature. We first use the most commonly used change metrics to build simple unsupervised models. Then, we compare these unsupervised models with the state-of-The-Art supervised models under cross-validation, time-wise-cross-validation, and across-project prediction set-tings to determine whether they are of practical value. The experimental results, from open-source software systems, show that many simple unsupervised models perform better than the state-of-The-Art supervised models in effort-Aware JIT defect prediction.},
	author = {Yang, Yibiao and Zhou, Yuming and Liu, Jinping and Zhao, Yangyang and Lu, Hongmin and Xu, Lei and Xu, Baowen and Leung, Hareton},
	doi = {10.1145/2950290.295035},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Effort-Aware Just-in-Time Defect Prediction - Simple Unsupervised Models Could Be Better Than Supervised Models.pdf:pdf},
	isbn = {9781450342186},
	journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
	keywords = {Changes,Defect,EffOrt-Aware,Just-In-Time,Prediction},
	mendeley-groups = {Approach,Other Studies},
	pages = {157--168},
	title = {{Effort-Aware just-in-Time defect prediction: Simple unsupervised models could be better than supervised models}},
	volume = {13-18-Nove},
	year = {2016}
}
@article{Huang2017,
	abstract = {Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find more defective changes than a state-of-the-art supervised model (i.e., EALR). This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.'s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners' patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS. When compared with EALR, CBS detects about 15{\%} more defective changes and also significantly improves Precision and F1-score. When compared with LT, CBS achieves similar results in terms of Recall, but it significantly reduces context switches and false alarms before first success. Finally, we also discuss the implications of our findings for practitioners and researchers.},
	author = {Huang, Qiao and Xia, Xin and Lo, David},
	doi = {10.1109/ICSME.201751},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Supervised vs Unsupervised Models - A Holistic Look at Effort-Aware Just-in-Time Defect Prediction.pdf:pdf},
	isbn = {9781538609927},
	journal = {Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017},
	keywords = {Bias,Change classification,Cost effectiveness,Evaluation},
	mendeley-groups = {Approach},
	pages = {159--170},
	title = {{Supervised vs unsupervised models: A holistic look at effort-aware just-in-time defect prediction}},
	year = {2017}
}
@article{Syed2019,
	abstract = {Some of the challenges faced with Just-in-time defect (JIT) prediction are achieving high performing models and obtaining large quantities of labelled data. There is also a limited number of studie ...},
	author = {Syed, Arsalan},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Investigating the Practicality of Just-in-time Defect Prediction with Semi-supervised Learning on Industrial Commit Data.pdf:pdf},
	keywords = {Computer and Information Sciences,Data,och informationsvetenskap},
	mendeley-groups = {Other Studies},
	title = {{Investigating the Practicality of Just-in-time Defect Prediction with Semi-supervised Learning on Industrial Commit Data}},
	url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2{\%}3A1336751{\&}dswid=28},
	year = {2019}
}
@book{Chacon:2014:PG:2695634,
	address = {Berkely, CA, USA},
	author = {Chacon, Scott and Straub, Ben},
	edition = {2nd},
	isbn = {1484200772, 9781484200773},
	publisher = {Apress},
	title = {{Pro Git}},
	year = {2014}
}
@article{Kim2013,
	abstract = {Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (Par), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated Par on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. Par successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs. {\textcopyright} 2013 IEEE.},
	author = {Kim, Dongsun and Nam, Jaechang and Song, Jaewoo and Kim, Sunghun},
	doi = {10.1109/ICSE.2013.6606626},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Automatic Patch Generation Learned from Human-Written Patches.pdf:pdf},
	isbn = {9781467330763},
	issn = {02705257},
	journal = {Proceedings - International Conference on Software Engineering},
	number = {c},
	pages = {802--811},
	title = {{Automatic patch generation learned from human-written patches}},
	volume = {1},
	year = {2013}
}
@inproceedings{Malhotra2017,
	abstract = {Bug tracking and analysis truly remains one of the most active areas of software engineering research. Bug tracking results may be employed by the software practitioners of large software projects effectively. The cost of detecting and correcting the defect becomes exponentially higher as we go from requirement analysis to the maintenance phase, where defects might even lead to loss of lives. Software metrics in conjunction with defect data can serve as basis for developing predictive models. Open source projects which encompass contributions from millions of people provide capacious dataset for testing. There have been diverse machine learning techniques proposed in the literature for analyzing complex relationships and extracting useful information from problems using optimal resources and time. However, more extensive research comparing these techniques is needed to establish superiority of one technique over another. This study aims at comparison of 14 ML techniques for development of effective defect prediction models. The issues addressed are 1) Construction of automated tool in Java to collect OO, inheritance and other metrics and detect bugs in classes extracted from open source repository, 2) Use of relevant performance measures to evaluate performance of predictive models to detect bugs in classes, 3) Statistical tests to compare predictive capability of different machine learning techniques, 4) Validation of defect prediction models. The results of the study show that Single Layer Perceptron is the best technique amongst all the techniques used in this study for development of defect prediction models. The conclusions drawn from this study can be used for practical applications by software practitioners to determine best technique for defect prediction and consequently carry out effective allocation of resources.},
	author = {Malhotra, Ruchikan and Bahl, Laavanye and Sehgal, Sushant and Priya, Pragati},
	booktitle = {Proceedings of the 2017 International Conference On Big Data Analytics and Computational Intelligence, ICBDACI 2017},
	doi = {10.1109/ICBDACI.2017.8070806},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Empirical comparison of machine learning algorithms for bug prediction in open source software.pdf:pdf},
	isbn = {9781509063994},
	keywords = {Bug prediction,Machine learning,Object-oriented metrics,Open source software,Software reliability},
	mendeley-groups = {Other Studies},
	pages = {40--45},
	title = {{Empirical comparison of machine learning algorithms for bug prediction in open source software}},
	year = {2017}
}
@article{Punitha2013,
	abstract = {Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy. {\textcopyright} 2013 IEEE.},
	author = {Punitha, K. and Chitra, S.},
	doi = {10.1109/ICICES.2013.6508369},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Software defect prediction using software metrics - A survey.pdf:pdf},
	isbn = {9781467357869},
	journal = {2013 International Conference on Information Communication and Embedded Systems, ICICES 2013},
	keywords = {Software defect prediction,machine learning,scheme evaluation,software defectproneness prediction},
	mendeley-groups = {Other Studies},
	pages = {555--558},
	publisher = {IEEE},
	title = {{Software defect prediction using software metrics - A survey}},
	year = {2013}
}
@article{nist,
	author = {Tassey and Gregory},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/The Economic Impacts of inadequate Infrastructure for Software Testing.pdf:pdf},
	title = {{The Economic Impacts of Inadequate Infrastructure for Software Testing}},
	url = {https://www.nist.gov/system/files/documents/director/planning/report02-3.pdf},
	year = {2002}
}
@article{Catolino2019,
	abstract = {Bug Prediction is an activity aimed at identifying defect-prone source code entities that allows developers to focus testing efforts on specific areas of software systems. Recently, the research community proposed Just-in-Time (JIT) Bug Prediction with the goal of detecting bugs at commit-level. While this topic has been extensively investigated in the context of traditional systems, to the best of our knowledge, only a few preliminary studies assessed the performance of the technique in a mobile environment, by applying the metrics proposed by Kamei et al. in a within-project scenario. The results of these studies highlighted that there is still room for improvement. In this paper, we faced this problem to understand (i) which Kamei et al.'s metrics are useful in the mobile context, (ii) if different classifiers impact the performance of cross-project JIT bug prediction models and (iii) whether the application of ensemble techniques improves the capabilities of the models. To carry out the experiment, we first applied a feature selection technique, i.e., InfoGain, to filter relevant features and avoid models multicollinearity. Then, we assessed and compared the performance of four different well-known classifiers and four ensemble techniques. Our empirical study involved 14 apps and 42, 543 commits extracted from the COMMIT GURU platform. The results show that Naive Bayes achieves the best performance with respect to the other classifiers and in some cases outperforms some well-known ensemble techniques.},
	author = {Catolino, Gemma and {Di Nucci}, Dario and Ferrucci, Filomena},
	doi = {10.1109/mobilesoft.2019.00023},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Cross-Project Just-in-Time Bug Prediction for Mobile Apps - An empirical Assessment.pdf:pdf},
	journal = {2019 IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems (MOBILESoft)},
	mendeley-groups = {Other Studies},
	pages = {99--110},
	publisher = {IEEE},
	title = {{Cross-Project Just-in-Time Bug Prediction for Mobile Apps: An Empirical Assessment}},
	year = {2019}
}
@book{Martin08,
	abstract = {Even bad code can function. But if code isn't clean, it can bring a development organization to its knees. Every year, countless hours and significant resources are lost because of poorly written code. But it doesn't have to be that way. Noted software expert Robert C. Martin presents a revolutionary paradigm with Clean Code: A Handbook of Agile Software Craftsmanship. Martin has teamed up with his colleagues from Object Mentor to distill their best agile practice of cleaning code on the fly into a book that will instill within you the values of a software craftsman and make you a better programmer---but only if you work at it. What kind of work will you be doing? You'll be reading code---lots of code. And you will be challenged to think about what's right about that code, and what's wrong with it. More importantly, you will be challenged to reassess your professional values and your commitment to your craft. Clean Code is divided into three parts. The first describes the principles, patterns, and practices of writing clean code. The second part consists of several case studies of increasing complexity. Each case study is an exercise in cleaning up code---of transforming a code base that has some problems into one that is sound and efficient. The third part is the payoff: a single chapter containing a list of heuristics and smells gathered while creating the case studies. The result is a knowledge base that describes the way we think when we write, read, and clean code.},
	address = {Upper Saddle River, NJ},
	author = {Martin, Robert C},
	isbn = {978-0-13235-088-4},
	keywords = {01841 105 book shelf safari agile software develop},
	publisher = {Prentice Hall},
	series = {Robert C. Martin Series},
	title = {{Clean Code: A Handbook of Agile Software Craftsmanship}},
	url = {https://www.safaribooksonline.com/library/view/clean-code/9780136083238/},
	year = {2008}
}
@article{Kim2008,
	abstract = {This paper introduces a new technique for finding latent software bugs called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes, or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project, as stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean with 78 percent accuracy and 65 percent buggy change recall (on average). Change classification has several desirable qualities: (1) the prediction granularity is small (a change to a single file), (2) predictions do not require semantic information about the source code, (3) the technique works for a broad array of project types and programming languages, and (4) predictions can be made immediately upon completion of a change. Contributions of the paper include a description of the change classification approach, techniques for extracting features from source code and change histories, a characterization of the performance of change classification across 12 open source projects, and evaluation of the predictive power of different groups of features. {\textcopyright} 2008 IEEE.},
	author = {Kim, Sunghun and Whitehead, E. James and Zhang, Yi},
	doi = {10.1109/TSE.2007.70773},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Classifying Software Changes - Clean or Buggy.pdf:pdf},
	issn = {00985589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {Association rules,Classification,Configuration management,Data mining,Machine learning,Maintenance,Software fault diagnosis,Software metrics},
	mendeley-groups = {Other Studies},
	number = {2},
	pages = {181--196},
	title = {{Classifying software changes: Clean or buggy?}},
	volume = {34},
	year = {2008}
}
@article{Kitchenham2009,
	abstract = {Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	author = {Kitchenham, Barbara and {Pearl Brereton}, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
	doi = {10.1016/j.infsof.2008.09.009},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Systematic literature reviews in software engineering â€“ A systematic literatur review.pdf:pdf},
	issn = {09505849},
	journal = {Information and Software Technology},
	keywords = {Cost estimation,Evidence-based software engineering,Systematic literature review,Systematic review quality,Tertiary study},
	mendeley-groups = {On Studies},
	number = {1},
	pages = {7--15},
	publisher = {Elsevier B.V.},
	title = {{Systematic literature reviews in software engineering - A systematic literature review}},
	url = {http://dx.doi.org/10.1016/j.infsof.2008.09.009},
	volume = {51},
	year = {2009}
}
@article{Powers2007,
	abstract = {Commonly used evaluation measures including Recall, Precision, F-Factor and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case. .},
	author = {Powers, David M W},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Evaluation - From Precision, Recall and F-Factor to ROC, Markedness and Correlation.pdf:pdf},
	journal = {Human Communication Science SummerFest},
	keywords = {bookmaker informedness and markedness,chi-squared significance,cohen kappa,evenness,f-factor,log-likelihood,matthews correlation,pearson correlation,precision,rand accuracy,recall,significance},
	number = {December},
	pages = {24},
	title = {{Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness {\&} Correlation}},
	year = {2007}
}
@article{Fawcett2006,
	abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
	author = {Fawcett, Tom},
	doi = {10.1016/j.patrec.2005.10.010},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/An introduction to ROC analysis.pdf:pdf},
	issn = {01678655},
	journal = {Pattern Recognition Letters},
	keywords = {Classifier evaluation,Evaluation metrics,ROC analysis},
	number = {8},
	pages = {861--874},
	title = {{An introduction to ROC analysis}},
	volume = {27},
	year = {2006}
}
@article{Pan2009,
	abstract = {Twenty-seven automatically extractable bug fix patterns are defined using the syntax components and context of the source code involved in bug fix changes. Bug fix patterns are extracted from the configuration management repositories of seven open source projects, all written in Java (Eclipse, Columba, JEdit, Scarab, ArgoUML, Lucene, and MegaMek). Defined bug fix patterns cover 45.7{\%} to 63.3{\%} of the total bug fix hunk pairs in these projects. The frequency of occurrence of each bug fix pattern is computed across all projects. The most common individual patterns are MC-DAP (method call with different actual parameter values) at 14.9-25.5{\%}, IF-CC (change in if conditional) at 5.6-18.6{\%}, and AS-CE (change of assignment expression) at 6.0-14.2{\%}. A correlation analysis on the extracted pattern instances on the seven projects shows that six have very similar bug fix pattern frequencies. Analysis of if conditional bug fix sub-patterns shows a trend towards increasing conditional complexity in if conditional fixes. Analysis of five developers in the Eclipse projects shows overall consistency with project-level bug fix pattern frequencies, as well as distinct variations among developers in their rates of producing various bug patterns. Overall, data in the paper suggest that developers have difficulty with specific code situations at surprisingly consistent rates. There appear to be broad mechanisms causing the injection of bugs that are largely independent of the type of software being produced. {\textcopyright} 2008 Springer Science+Business Media, LLC.},
	author = {Pan, Kai and Kim, Sunghun and Whitehead, E. James},
	doi = {10.1007/s10664-008-9077-5},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Toward an understanding of bug fix patterns.pdf:pdf},
	issn = {13823256},
	journal = {Empirical Software Engineering},
	keywords = {Algorithms,Bug fix changes,Categorization of software faults,Causes of software bugs,Experimentation,Measurement,Software bugs,Software error,Software fault,Software fault taxonomy},
	number = {3},
	pages = {286--315},
	title = {{Toward an understanding of bug fix patterns}},
	volume = {14},
	year = {2009}
}
@article{Just2014a,
	abstract = {A good test suite is one that detects real faults. Because the set of faults in a program is usually unknowable, this definition is not useful to practitioners who are creating test suites, nor to researchers who are creating and evaluating tools that generate test suites. In place of real faults, testing research often uses mutants, which are artificial faults - each one a simple syntactic variation - that are systematically seeded throughout the program under test. Mutation analysis is appealing because large numbers of mutants can be automatically-generated and used to compensate for low quantities or the absence of known real faults. Unfortunately, there is little experimental evidence to support the use of mutants as a replacement for real faults. This paper investigates whether mutants are indeed a valid substitute for real faults, i.e., whether a test suite's ability to detect mutants is correlated with its ability to detect real faults that developers have fixed. Unlike prior studies, these investigations also explicitly consider the conflating effects of code coverage on the mutant detection rate. Our experiments used 357 real faults in 5 open-source applications that comprise a total of 321,000 lines of code. Furthermore, our experiments used both developer-written and automaticallygenerated test suites. The results show a statistically significant correlation between mutant detection and real fault detection, independently of code coverage. The results also give concrete suggestions on how to improve mutation analysis and reveal some inherent limitations.},
	author = {Just, Ren{\'{e}} and Jalali, Darioush and Inozemtseva, Laura and Ernst, Michael D. and Holmes, Reid and Fraser, Gordon},
	doi = {10.1145/2635868.2635929},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Are Mutants a Valid Substitute for Real Faults in Software Testing.pdf:pdf},
	isbn = {9781450330565},
	journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
	keywords = {Code coverage,Mutation analysis,Real faults,Test effectiveness},
	mendeley-groups = {Related Information},
	pages = {654--665},
	title = {{Are mutants a valid substitute for real faults in software testing?}},
	volume = {16-21-Nove},
	year = {2014}
}
@techreport{JohnN1969,
	author = {{John N}, Buxton and Randell, Brian},
	booktitle = {NATO SOFTWARE ENGINEERING CONFERENCE 1969},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/SOFTWARE ENGINEERING TECHNIQUES.PDF:PDF},
	pages = {16},
	title = {{Software Engineering Techniques}},
	url = {http://homepages.cs.ncl.ac.uk/brian.randell/NATO/nato1969.PDF},
	year = {1969}
}
@article{Maayan2018,
	author = {Ma'ayan, Dor D.},
	doi = {10.1145/3194095.3194102},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/The Quality of Junit Tests.pdf:pdf},
	isbn = {9781450357371},
	journal = {2018 IEEE/ACM 1st International Workshop on Software Qualities and their Dependencies (SQUADE)},
	keywords = {acm reference format,algebraic representation,omy,software engineering,software metrics,taxon-,testing},
	pages = {33--36},
	publisher = {ACM},
	title = {{The quality of junit tests}},
	year = {2018}
}
@misc{Just2019,
	author = {Just, Ren{\'{e}}},
	mendeley-groups = {Related Information},
	title = {{defects4j GitHub Repository}},
	url = {https://github.com/rjust/defects4j},
	urldate = {2019-11-04},
	year = {2019}
}
@article{Kiehn2019,
	abstract = {Many techniques have been proposed for mining software repositories, predicting code quality and evaluating code changes. Prior work has established links between code ownership and churn metrics, and software quality at file and directory level based on changes that fix bugs. Other metrics have been used to evaluate individual code changes based on preceding changes that induce fixes. This paper combines the two approaches in an empirical study of assessing risk of code changes using established code ownership and churn metrics with fix inducing changes on a large proprietary code repository. We establish a machine learning model for change risk classification which achieves average precision of 0.76 using metrics from prior works and 0.90 using a wider array of metrics. Our results suggest that code ownership metrics can be applied in change risk classification models based on fix inducing changes.},
	author = {Kiehn, Max and Pan, Xiangyi and Camci, Fatih},
	doi = {10.1109/MSR.2019.00018},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Empirical Study in using Version Histories for Change Risk Classification.pdf:pdf},
	isbn = {9781728134123},
	issn = {21601860},
	journal = {IEEE International Working Conference on Mining Software Repositories},
	keywords = {Change risk,Code ownership,File metrics,Machine learning},
	mendeley-groups = {Other Studies},
	pages = {58--62},
	publisher = {IEEE},
	title = {{Empirical study in using version histories for change risk classification}},
	volume = {2019-May},
	year = {2019}
}
@article{Zhu2018,
	abstract = {Bug prediction in software code changes can help developers to find out and fix bugs immediately when they are introduced, thus to improve the effectiveness and validity of bug fixing. In data mining, this problem can be regarded as a change classification task. However, one of its key characteristics, ie, class-imbalance, holds back the performance of standard classification methods. In this paper, we consider a quantity of imbalance data-handling methods and extract a more comprehensive groups of change features, aiming to achieve better change classification performance. Two different types of imbalance data-handling methods, namely, resampling and ensemble learning methods, are employed. Especially, we explore the performance of their combination. To compare the performance of different imbalance data-handling methods, an experiment with 10 open source projects is conducted. Four classification methods, including J48, Na{\"{i}}ve Bayes, SMO, and Random Forest, are used as standard classifiers and as the base classifiers, respectively. Moreover, contribution of different groups of change features are evaluated. Experimental results show that imbalance data-handling methods can improve the performance of change classification and the combination methods, which take advantage of both ensemble learning and resampling, perform better than using ensemble learning methods or resampling methods individually. Of the studied imbalance data-handling methods, the combination of Bagging and random undersampling with J48 as the base classifier yields out better prediction results than those achieved by other methods. Additionally, of the collected change features, text vector features accounts for a larger proportion than others.},
	author = {Zhu, Xiaoyan and Niu, Binbin and Whitehead, E. James and Sun, Zhongbin},
	doi = {10.1002/spe.2606},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/An empirical study of software change classification withimbalance data-handling methods.pdf:pdf},
	issn = {1097024X},
	journal = {Software - Practice and Experience},
	keywords = {bug prediction,change classification,ensemble learning,imbalance data,resampling},
	mendeley-groups = {Other Studies},
	number = {11},
	pages = {1968--1999},
	title = {{An empirical study of software change classification with imbalance data-handling methods}},
	volume = {48},
	year = {2018}
}
@article{Just2014,
	abstract = {Empirical studies in software testing research may not be comparable, reproducible, or characteristic of practice. One reason is that real bugs are too infrequently used in software testing research. Extracting and reproducing real bugs is challenging and as a result hand-seeded faults or mutants are commonly used as a substitute. This paper presents Defects4J, a database and extensible framework providing real bugs to enable reproducible studies in software testing research. The initial version of Defects4J contains 357 real bugs from 5 real-world open source pro- grams. Each real bug is accompanied by a comprehensive test suite that can expose (demonstrate) that bug. Defects4J is extensible and builds on top of each program's version control system. Once a program is configured in Defects4J, new bugs can be added to the database with little or no effort. Defects4J features a framework to easily access faulty and fixed program versions and corresponding test suites. This framework also provides a high-level interface to common tasks in software testing research, making it easy to con- duct and reproduce empirical studies. Defects4J is publicly available at http://defects4j.org.},
	author = {Just, Ren{\'{e}} and Jalali, Darioush and Ernst, Michael D.},
	doi = {10.1145/2610384.2628055},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Defects4J - A database for existing faults to enable controlled testing studies for java programmers.pdf:pdf},
	isbn = {9781450326452},
	journal = {2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
	keywords = {Bug database,Real bugs,Testing framework},
	mendeley-groups = {Related Information},
	pages = {437--440},
	title = {{Defects4J: A database of existing faults to enable controlled testing studies for Java programs}},
	year = {2014}
}
@article{10.2307/2237615,
	abstract = {An s stage k name snowball sampling procedure is defined as follows: A random sample of individuals is drawn from a given finite population. (The kind of random sample will be discussed later in this section.) Each individual in the sample is asked to name k different individuals in the population, where k is a specified integer; for example, each individual may be asked to name his "k best friends," or the "k individuals with whom he most frequently associates," or the "k individuals whose opinions he most frequently seeks," etc. (For the sake of simplicity, we assume throughout that an individual cannot include himself in his list of k individuals.) The individuals who were not in the random sample but were named by individuals in it form the first stage. Each of the individuals in the first stage is then asked to name k different individuals. (We assume that the question asked of the individuals in the random sample and of those in each stage is the same and that k is the same.) The individuals who were not in the random sample nor in the first stage but were named by individuals who were in the first stage form the second stage. Each of the individuals in the second stage is then asked to name k different individuals. The individuals who were not in the random sample nor in the first or second stages but were named by individuals who were in the second stage form the third stage. Each of the individuals in the third stage is then asked to name k different individuals. This procedure is continued until each of the individuals in the sth stage has been asked to name k different individuals. The data obtained using an s stage k name snowball sampling procedure can be utilized to make statistical inferences about various aspects of the relationships present in the population. The relationships present, in the hypothetical situation where each individual in the population is asked to name k different individuals, can be described by a matrix with rows and columns corresponding to the members of the population, rows for the individuals naming and columns for the individuals named, where the entry $\theta$ij in the ith row and jth column is 1 if the ith individual in the population includes the jth individual among the k individuals he would name, and it is 0 otherwise. While the matrix of the $\theta$'s cannot be known in general unless every individual in the population is interviewed (i.e., asked to name k different individuals), it will be possible to make statistical inferences about various aspects of this matrix from the data obtained using an s stage k name snowball sampling procedure. For example, when s = k = 1, the number, M11, of mutual relationships present in the population (i.e., the number of values i with $\theta$ij = $\theta$ji = 1 for some value of {\$}j {\textgreater} i{\$}) can be estimated. The methods of statistical inference applied to the data obtained from an s stage k name snowball sample will of course depend on the kind of random sample drawn as the initial step. In most of the present paper, we shall suppose that a random sample (i.e., the "zero stage" in snowball sample) is drawn so that the probability, p, that a given individual in the population will be in the sample is independent of whether a different given individual has appeared. This kind of sampling has been called binomial sampling; the specified value of p (assumed known) has been called the sampling fraction [4]. This sampling scheme might also be described by saying that a given individual is included in the sample just when a coin, which has a probability p of "heads," comes up "heads," where the tosses of the coin from individual to individual are independent. (To each individual there corresponds an independent Bernoulli trial determining whether he will or will not be included in the sample.) This sampling scheme differs in some respects from the more usual models where the sample size is fixed in advance or where the ratio of the sample size to the population size (i.e., the sample size-population size ratio) is fixed. For binomial sampling, this ratio is a random variable whose expected value is p. (The variance of this ratio approaches zero as the population becomes infinite.) In some situations (where, for example, the variance of this ratio is near zero), mathematical results obtained for binomial sampling are sometimes quite similar to results obtained using some of the more usual sampling models (see [4], [7]; compare the variance formulas in [3] and [5]); in such cases it will often not make much difference, from a practical point of view, which sampling model is utilized. (In Section 6 of the present paper some results for snowball sampling based on an initial sample of the more usual kind are obtained and compared with results presented in the earlier sections of this paper obtained for snowball sampling based on an initial binomial sample.) For snowball sampling based on an initial binomial sample, and with s = k = 1, so that each individual asked names just one other individual and there is just one stage beyond the initial sample, Section 2 of this paper discusses unbiased estimation of M11, the number of pairs of individuals in the population who would name each other. One of the unbiased estimators considered (among a certain specified class of estimators) has uniformly smallest variance when the population characteristics are unknown; this one is based on a sufficient statistic for a simplified summary of the data and is the only unbiased estimator of M11 based on that sufficient statistic (when the population characteristics are unknown). This estimator (when s = k = 1) has a smaller variance than a comparable minimum variance unbiased estimator computed from a larger random sample when s = 0 and k = 1 (i.e., where only the individuals in the random sample are interviewed) even where the expected number of individuals in the larger random sample (s = 0, k = 1) is equal to the maximum expected number of individuals studied when s = k = 1 (i.e., the sum of the expected number of individuals in the initial sample and the maximum expected number of individuals in the first stage). In fact, the variance of the estimator when s = 0 and k = 1 is at least twice as large as the variance of the comparable estimator when s = k = 1 even where the expected number of individuals studied when s = 0 and k = 1 is as large as the maximum expected number of individuals studied when s = k = 1. Thus, for estimating M11, the sampling scheme with s = k = 1 is preferable to the sampling scheme with s = 0 and k = 1. Furthermore, we observe that when s = k = 1 the unbiased estimator based on the simplified summary of the data having minimum variance when the population characteristics are unknown can be improved upon in cases where certain population characteristics are known, or where additional data not included in the simplified summary are available. Several improved estimators are derived and discussed. Some of the results for the special case of s = k = 1 are generalized in Sections 3 and 4 to deal with cases where s and k are any specified positive integers. In Section 5, results are presented about s stage k name snowball sampling procedures, where each individual asked to name k different individuals chooses k individuals at random from the population. (Except in Section 5, the numbers $\theta$ij, which form the matrix referred to earlier, are assumed to be fixed (i.e., to be population parameters); in Section 5, they are random variables. A variable response error is not considered except in so far as Section 5 deals with an extreme case of this.) For social science literature that discusses problems related to snowball sampling, see [2], [8], and the articles they cite. This literature indicates, among other things, the importance of studying "social structure and...the relations among individuals" [2].},
	author = {Goodman, Leo A},
	issn = {00034851},
	journal = {The Annals of Mathematical Statistics},
	number = {1},
	pages = {148--170},
	publisher = {Institute of Mathematical Statistics},
	title = {{Snowball Sampling}},
	url = {http://www.jstor.org/stable/2237615},
	volume = {32},
	year = {1961}
}
@article{Rosen2015,
	abstract = {Software quality is one of the most important research sub-areas of software engineering. Hence, a plethora of research has focused on the prediction of software quality. Much of the software analytics and prediction work has proposed metrics, models and novel approaches that can predict quality with high levels of accuracy. However, adoption of such techniques remain low; one of the reasons for this low adoption of the current analytics and prediction technique is the lack of actionable and publicly available tools. We present Commit Guru, a language agnostic analytics and prediction tool that identifies and predicts risky software commits. Commit Guru is publicly available and is able to mine any GIT SCM repository. Analytics are generated at both, the project and commit levels. In addition, Commit Guru automatically identifies risky (i.e., bug-inducing) commits and builds a prediction model that assess the likelihood of a recent commit introducing a bug in the future. Finally, to facilitate future research in the area, users of Commit Guru can download the data for any project that is processed by Commit Guru with a single click. Several large open source projects have been successfully processed using Commit Guru. Commit Guru is available online at commit.guru. Our source code is also released freely under the MIT license.},
	author = {Rosen, Christoffer and Grawi, Ben and Shihab, Emad},
	doi = {10.1145/2786805.2803183},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Commit Guru - Analytics and Risk Prediction.pdf:pdf},
	isbn = {9781450336758},
	journal = {2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},
	keywords = {Risky software commits,Software analytics,Software metrics,Software prediction},
	mendeley-groups = {Primary Approaches},
	pages = {966--969},
	title = {{Commit guru: Analytics and risk prediction of software commits}},
	year = {2015}
}
@article{Goyal2017,
	abstract = {Transparent environments and social-coding platforms as GitHub help developers to stay abreast of changes during the development and maintenance phase of a project. Especially, notification feeds can help developers to learn about relevant changes in other projects. Unfortunately, transparent environments can quickly overwhelm developers with too many notifications, such that they lose the important ones in a sea of noise. Complementing existing prioritization and filtering strategies based on binary compatibility and code ownership, we develop an anomaly detection mechanism to identify unusual commits in a repository, which stand out with respect to other changes in the same repository or by the same developer. Among others, we detect exceptionally large commits, commits at unusual times, and commits touching rarely changed file types given the characteristics of a particular repository or developer. We automatically flag unusual commits on GitHub through a browser plug-in. In an interactive survey with 173 active GitHub users, rating commits in a project of their interest, we found that, although our unusual score is only a weak predictor of whether developers want to be notified about a commit, information about unusual characteristics of a commit changes how developers regard commits. Our anomaly detection mechanism is a building block for scaling transparent environments.},
	author = {Goyal, Raman and Ferreira, Gabriel and K{\"{a}}stner, Christian and Herbsleb, James},
	doi = {10.1002/smr.1893},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Identifying unusual commits on GitHub.pdf:pdf},
	issn = {20477481},
	journal = {Journal of Software: Evolution and Process},
	keywords = {Anomaly detection,Information overload,Notification feeds,Software ecosystems,Transparent environments},
	mendeley-groups = {Approach,Primary Approaches},
	number = {August 2017},
	pages = {1--16},
	title = {{Identifying unusual commits on GitHub}},
	year = {2017}
}
@article{Nayrolles2018,
	abstract = {Automatic prevention and resolution of faults is an important research topic in the field of software maintenance and evolution. Existing approaches leverage code and process metrics to build metric-based models that can effectively prevent defect insertion in a software project. Metrics, however, may vary from one project to another, hindering the reuse of these models. Moreover, they tend to generate high false positive rates by classifying healthy commits as risky. Finally, they do not provide sufficient insights to developers on how to fix the detected risky commits. In this paper, we propose an approach, called CLEVER (Combining Levels of Bug Prevention and Resolution techniques), which relies on a two-phase process for intercepting risky commits before they reach the central repository. When applied to 12 Ubisoft systems, the results show that CLEVER can detect risky commits with 79{\%} precision and 65{\%} recall, which outperforms the performance of Commit-guru, a recent approach that was proposed in the literature. In addition, CLEVER is able to recommend qualitative fixes to developers on how to fix risky commits in 66.7{\%} of the cases.},
	author = {Nayrolles, Mathieu and Hamou-Lhadj, Abdelwahab},
	doi = {10.1145/3196398.3196438},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/CLEVER - Combining Code Metrics with Clone Detection for Just-In-Time Fault Prevention and Resolution in Large Industrial Projects.pdf:pdf},
	isbn = {9781450357166},
	issn = {02705257},
	journal = {Proceedings - International Conference on Software Engineering},
	keywords = {defect predictions,fault fixing,software evolution,software maintenance},
	mendeley-groups = {Approach,Primary Approaches},
	pages = {153--164},
	publisher = {ACM},
	title = {{CLEVER: Combining code metrics with clone detection for just-in-time fault prevention and resolution in large industrial projects}},
	year = {2018}
}
