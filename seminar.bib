@article{Hata2018,
	abstract = {Bug fixing is generally a manually-intensive task. However, recent work has proposed the idea of automated program repair, which aims to repair (at least a subset of) bugs in different ways such as code mutation, etc. Following in the same line of work as automated bug repair, in this paper we aim to leverage past fixes to propose fixes of current/future bugs. Specifically, we propose Ratchet, a corrective patch generation system using neural machine translation. By learning corresponding pre-correction and post-correction code in past fixes with a neural sequence-to-sequence model, Ratchet is able to generate a fix code for a given bug-prone code query. We perform an empirical study with five open source projects, namely Ambari, Camel, Hadoop, Jetty and Wicket, to evaluate the effectiveness of Ratchet. Our findings show that Ratchet can generate syntactically valid statements 98.7{\%} of the time, and achieve an F1-measure between 0.29 - 0.83 with respect to the actual fixes adopted in the code base. In addition, we perform a qualitative validation using 20 participants to see whether the generated statements can be helpful in correcting bugs. Our survey showed that Ratchet's output was considered to be helpful in fixing the bugs on many occasions, even if fix was not 100{\%} correct.},
	archivePrefix = {arXiv},
	arxivId = {1812.07170},
	author = {Hata, Hideaki and Shihab, Emad and Neubig, Graham},
	eprint = {1812.07170},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Learning to Generate Corrective Patches Using Neural Machine Translation.pdf:pdf},
	mendeley-groups = {Approach},
	number = {8},
	pages = {1--20},
	title = {{Learning to Generate Corrective Patches using Neural Machine Translation}},
	url = {http://arxiv.org/abs/1812.07170},
	volume = {14},
	year = {2018}
}
@article{Catolino2019,
	abstract = {Bug Prediction is an activity aimed at identifying defect-prone source code entities that allows developers to focus testing efforts on specific areas of software systems. Recently, the research community proposed Just-in-Time (JIT) Bug Prediction with the goal of detecting bugs at commit-level. While this topic has been extensively investigated in the context of traditional systems, to the best of our knowledge, only a few preliminary studies assessed the performance of the technique in a mobile environment, by applying the metrics proposed by Kamei et al. in a within-project scenario. The results of these studies highlighted that there is still room for improvement. In this paper, we faced this problem to understand (i) which Kamei et al.'s metrics are useful in the mobile context, (ii) if different classifiers impact the performance of cross-project JIT bug prediction models and (iii) whether the application of ensemble techniques improves the capabilities of the models. To carry out the experiment, we first applied a feature selection technique, i.e., InfoGain, to filter relevant features and avoid models multicollinearity. Then, we assessed and compared the performance of four different well-known classifiers and four ensemble techniques. Our empirical study involved 14 apps and 42, 543 commits extracted from the COMMIT GURU platform. The results show that Naive Bayes achieves the best performance with respect to the other classifiers and in some cases outperforms some well-known ensemble techniques.},
	author = {Catolino, Gemma and {Di Nucci}, Dario and Ferrucci, Filomena},
	doi = {10.1109/mobilesoft.2019.00023},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Cross-Project Just-in-Time Bug Prediction for Mobile Apps - An empirical Assessment.pdf:pdf},
	journal = {2019 IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems (MOBILESoft)},
	mendeley-groups = {Other Studies},
	pages = {99--110},
	publisher = {IEEE},
	title = {{Cross-Project Just-in-Time Bug Prediction for Mobile Apps: An Empirical Assessment}},
	year = {2019}
}
@misc{Just2019,
	author = {Just, Ren{\'{e}}},
	mendeley-groups = {Related Information},
	title = {{defects4j GitHub Repository}},
	url = {https://github.com/rjust/defects4j},
	urldate = {2019-11-04},
	year = {2019}
}
@article{Huang2017,
	abstract = {Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find more defective changes than a state-of-the-art supervised model (i.e., EALR). This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.'s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners' patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS. When compared with EALR, CBS detects about 15{\%} more defective changes and also significantly improves Precision and F1-score. When compared with LT, CBS achieves similar results in terms of Recall, but it significantly reduces context switches and false alarms before first success. Finally, we also discuss the implications of our findings for practitioners and researchers.},
	author = {Huang, Qiao and Xia, Xin and Lo, David},
	doi = {10.1109/ICSME.201751},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Supervised vs Unsupervised Models - A Holistic Look at Effort-Aware Just-in-Time Defect Prediction.pdf:pdf},
	isbn = {9781538609927},
	journal = {Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017},
	keywords = {Bias,Change classification,Cost effectiveness,Evaluation},
	mendeley-groups = {Approach},
	pages = {159--170},
	title = {{Supervised vs unsupervised models: A holistic look at effort-aware just-in-time defect prediction}},
	year = {2017}
}
@article{Bader2019,
	abstract = {Static analyzers help find bugs early by warning about recurring bug categories. While fixing these bugs still remains a mostly manual task in practice, we observe that fixes for a specific bug category often are repetitive. This paper addresses the problem of automatically fixing instances of common bugs by learning from past fixes. We present Getafix, an approach that produces human-like fixes while being fast enough to suggest fixes in time proportional to the amount of time needed to obtain static analysis results in the first place. Getafix is based on a novel hierarchical clustering algorithm that summarizes fix patterns into a hierarchy ranging from general to specific patterns. Instead of an expensive exploration of a potentially large space of candidate fixes, Getafix uses a simple yet effective ranking technique that uses the context of a code change to select the most appropriate fix for a given bug. Our evaluation applies Getafix to 1,268 bug fixes for six bug categories reported by popular static analyzers for Java, including null dereferences, incorrect API calls, and misuses of particular language constructs. The approach predicts exactly the human-written fix as the top-most suggestion between 12{\%} and 91{\%} of the time, depending on the bug category. The top-5 suggestions contain fixes for 526 of the 1,268 bugs. Moreover, we report on deploying the approach within Facebook, where it contributes to the reliability of software used by billions of people. To the best of our knowledge, Getafix is the first industrially-deployed automated bug-fixing tool that learns fix patterns from past, human-written fixes to produce human-like fixes.},
	annote = {Potentiall auch eine Art der "Commit Validation", allerdings mit dem Trigger "Code with static analysis warning".},
	archivePrefix = {arXiv},
	arxivId = {1902.06111},
	author = {Bader, Johannes and Scott, Andrew and Pradel, Michael and Chandra, Satish},
	eprint = {1902.06111},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Getafix - Learning to Fix Bugs Automatically.pdf:pdf},
	keywords = {approach,secondaryapproach},
	mendeley-groups = {Approach},
	mendeley-tags = {approach,secondaryapproach},
	title = {{Getafix: Learning to Fix Bugs Automatically}},
	url = {http://arxiv.org/abs/1902.06111},
	year = {2019}
}
@article{Yang2016,
	abstract = {Unsupervised models do not require the defect data to build the prediction models and hence incur a low building cost and gain a wide application range. Consequently, it would be more desirable for practitioners to apply unsupervised models in effort-Aware just-in-Time (JIT) defect prediction if they can predict defect-inducing changes well. However, little is currently known on their prediction effectiveness in this context. We aim to investigate the predictive power of simple unsupervised models in effort-Aware JIT defect prediction, especially compared with the state-of-The-Art su-pervised models in the recent literature. We first use the most commonly used change metrics to build simple unsupervised models. Then, we compare these unsupervised models with the state-of-The-Art supervised models under cross-validation, time-wise-cross-validation, and across-project prediction set-tings to determine whether they are of practical value. The experimental results, from open-source software systems, show that many simple unsupervised models perform better than the state-of-The-Art supervised models in effort-Aware JIT defect prediction.},
	author = {Yang, Yibiao and Zhou, Yuming and Liu, Jinping and Zhao, Yangyang and Lu, Hongmin and Xu, Lei and Xu, Baowen and Leung, Hareton},
	doi = {10.1145/2950290.295035},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Effort-Aware Just-in-Time Defect Prediction - Simple Unsupervised Models Could Be Better Than Supervised Models.pdf:pdf},
	isbn = {9781450342186},
	journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
	keywords = {Changes,Defect,EffOrt-Aware,Just-In-Time,Prediction},
	mendeley-groups = {Approach,Other Studies},
	pages = {157--168},
	title = {{Effort-Aware just-in-Time defect prediction: Simple unsupervised models could be better than supervised models}},
	volume = {13-18-Nove},
	year = {2016}
}
@article{Just2014,
	abstract = {Empirical studies in software testing research may not be comparable, reproducible, or characteristic of practice. One reason is that real bugs are too infrequently used in software testing research. Extracting and reproducing real bugs is challenging and as a result hand-seeded faults or mutants are commonly used as a substitute. This paper presents Defects4J, a database and extensible framework providing real bugs to enable reproducible studies in software testing research. The initial version of Defects4J contains 357 real bugs from 5 real-world open source pro- grams. Each real bug is accompanied by a comprehensive test suite that can expose (demonstrate) that bug. Defects4J is extensible and builds on top of each program's version control system. Once a program is configured in Defects4J, new bugs can be added to the database with little or no effort. Defects4J features a framework to easily access faulty and fixed program versions and corresponding test suites. This framework also provides a high-level interface to common tasks in software testing research, making it easy to con- duct and reproduce empirical studies. Defects4J is publicly available at http://defects4j.org.},
	author = {Just, Ren{\'{e}} and Jalali, Darioush and Ernst, Michael D.},
	doi = {10.1145/2610384.2628055},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Defects4J - A database for existing faults to enable controlled testing studies for java programmers.pdf:pdf},
	isbn = {9781450326452},
	journal = {2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
	keywords = {Bug database,Real bugs,Testing framework},
	mendeley-groups = {Related Information},
	pages = {437--440},
	title = {{Defects4J: A database of existing faults to enable controlled testing studies for Java programs}},
	year = {2014}
}
@article{Kim2008,
	abstract = {This paper introduces a new technique for finding latent software bugs called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes, or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project, as stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean with 78 percent accuracy and 65 percent buggy change recall (on average). Change classification has several desirable qualities: (1) the prediction granularity is small (a change to a single file), (2) predictions do not require semantic information about the source code, (3) the technique works for a broad array of project types and programming languages, and (4) predictions can be made immediately upon completion of a change. Contributions of the paper include a description of the change classification approach, techniques for extracting features from source code and change histories, a characterization of the performance of change classification across 12 open source projects, and evaluation of the predictive power of different groups of features. {\textcopyright} 2008 IEEE.},
	author = {Kim, Sunghun and Whitehead, E. James and Zhang, Yi},
	doi = {10.1109/TSE.2007.70773},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Classifying Software Changes - Clean or Buggy.pdf:pdf},
	issn = {00985589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {Association rules,Classification,Configuration management,Data mining,Machine learning,Maintenance,Software fault diagnosis,Software metrics},
	mendeley-groups = {Other Studies},
	number = {2},
	pages = {181--196},
	title = {{Classifying software changes: Clean or buggy?}},
	volume = {34},
	year = {2008}
}
@article{Nayrolles2018,
	abstract = {Software maintenance activities such as debugging and feature enhancement are known to be challenging and costly, which explains an ever growing line of research in software maintenance areas including mining software repository, default prevention, clone detection, and bug reproduction. The main goal is to improve the productivity of software developers as they undertake maintenance tasks. Existing tools, however,
	operate in an offline fashion, i.e., after the changes to the systems have been made.
	Studies have shown that software developers tend to be reluctant to use these tools as part of a continuous development process. This is because they require installation and training, hindering their integration with developers' workflow, which in turn limits their adoption. In this thesis, we propose novel approaches to support software developers at commit-time. As part of the developer's workflow, a commit marks the end of a given task. We show how commits can be used to catch unwanted modifications to the system, and prevent the introduction of clones and bugs, before these
	modifications reach the central code repository. We also propose a bug reproduction technique that is based on model checking and crash traces. Furthermore, we propose a new way for classifying bugs based on the location of fixes that can serve as the basis for future research in this field of study. The techniques proposed in this thesis have been tested on over 400 open and closed (industrial) systems, resulting in high levels of precision and recall. They are also scalable and non-intrusive.},
	author = {Nayrolles, Mathieu},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Software Maintenance at Commit Time.pdf:pdf},
	mendeley-groups = {Related Information},
	number = {August},
	title = {{Software Maintenance At Commit-Time}},
	year = {2018}
}
@article{Syed2019,
	abstract = {Some of the challenges faced with Just-in-time defect (JIT) prediction are achieving high performing models and obtaining large quantities of labelled data. There is also a limited number of studie ...},
	author = {Syed, Arsalan},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Investigating the Practicality of Just-in-time Defect Prediction with Semi-supervised Learning on Industrial Commit Data.pdf:pdf},
	keywords = {Computer and Information Sciences,Data,och informationsvetenskap},
	mendeley-groups = {Other Studies},
	title = {{Investigating the Practicality of Just-in-time Defect Prediction with Semi-supervised Learning on Industrial Commit Data}},
	url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2{\%}3A1336751{\&}dswid=28},
	year = {2019}
}
@article{Just2014a,
	abstract = {A good test suite is one that detects real faults. Because the set of faults in a program is usually unknowable, this definition is not useful to practitioners who are creating test suites, nor to researchers who are creating and evaluating tools that generate test suites. In place of real faults, testing research often uses mutants, which are artificial faults - each one a simple syntactic variation - that are systematically seeded throughout the program under test. Mutation analysis is appealing because large numbers of mutants can be automatically-generated and used to compensate for low quantities or the absence of known real faults. Unfortunately, there is little experimental evidence to support the use of mutants as a replacement for real faults. This paper investigates whether mutants are indeed a valid substitute for real faults, i.e., whether a test suite's ability to detect mutants is correlated with its ability to detect real faults that developers have fixed. Unlike prior studies, these investigations also explicitly consider the conflating effects of code coverage on the mutant detection rate. Our experiments used 357 real faults in 5 open-source applications that comprise a total of 321,000 lines of code. Furthermore, our experiments used both developer-written and automaticallygenerated test suites. The results show a statistically significant correlation between mutant detection and real fault detection, independently of code coverage. The results also give concrete suggestions on how to improve mutation analysis and reveal some inherent limitations.},
	author = {Just, Ren{\'{e}} and Jalali, Darioush and Inozemtseva, Laura and Ernst, Michael D. and Holmes, Reid and Fraser, Gordon},
	doi = {10.1145/2635868.2635929},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Are Mutants a Valid Substitute for Real Faults in Software Testing.pdf:pdf},
	isbn = {9781450330565},
	journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
	keywords = {Code coverage,Mutation analysis,Real faults,Test effectiveness},
	mendeley-groups = {Related Information},
	pages = {654--665},
	title = {{Are mutants a valid substitute for real faults in software testing?}},
	volume = {16-21-Nove},
	year = {2014}
}
@article{Koyuncu2019,
	abstract = {Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and-validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).},
	annote = {Gilt das auch als "Commit-Validation"?},
	archivePrefix = {arXiv},
	arxivId = {1907.05620},
	author = {Koyuncu, Anil and Liu, Kui and Bissyand{\'{e}}, Tegawend{\'{e}} F. and Kim, Dongsun and Monperrus, Martin and Klein, Jacques and {Le Traon}, Yves},
	doi = {10.1145/3338906.3338935},
	eprint = {1907.05620},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/iFixR - Bug Report driven Program Repair.pdf:pdf},
	isbn = {9781450355728},
	keywords = {2,acm reference format,anil koyuncu 1,automatic patch generation,bissyand{\'{e}} 1,dongsun kim 1,fault localization,information retrieval,kui liu 1,mar-,tegawend{\'{e}} f},
	mendeley-groups = {Approach},
	pages = {314--325},
	title = {{iFixR: bug report driven program repair}},
	year = {2019}
}
@article{Rosen2015,
	abstract = {Software quality is one of the most important research sub-areas of software engineering. Hence, a plethora of research has focused on the prediction of software quality. Much of the software analytics and prediction work has proposed metrics, models and novel approaches that can predict quality with high levels of accuracy. However, adoption of such techniques remain low; one of the reasons for this low adoption of the current analytics and prediction technique is the lack of actionable and publicly available tools. We present Commit Guru, a language agnostic analytics and prediction tool that identifies and predicts risky software commits. Commit Guru is publicly available and is able to mine any GIT SCM repository. Analytics are generated at both, the project and commit levels. In addition, Commit Guru automatically identifies risky (i.e., bug-inducing) commits and builds a prediction model that assess the likelihood of a recent commit introducing a bug in the future. Finally, to facilitate future research in the area, users of Commit Guru can download the data for any project that is processed by Commit Guru with a single click. Several large open source projects have been successfully processed using Commit Guru. Commit Guru is available online at commit.guru. Our source code is also released freely under the MIT license.},
	author = {Rosen, Christoffer and Grawi, Ben and Shihab, Emad},
	doi = {10.1145/2786805.2803183},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Commit Guru - Analytics and Risk Prediction.pdf:pdf},
	isbn = {9781450336758},
	journal = {2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},
	keywords = {Risky software commits,Software analytics,Software metrics,Software prediction},
	mendeley-groups = {Primary Approaches},
	pages = {966--969},
	title = {{Commit guru: Analytics and risk prediction of software commits}},
	year = {2015}
}
@article{Nayrolles2018,
	abstract = {Automatic prevention and resolution of faults is an important research topic in the field of software maintenance and evolution. Existing approaches leverage code and process metrics to build metric-based models that can effectively prevent defect insertion in a software project. Metrics, however, may vary from one project to another, hindering the reuse of these models. Moreover, they tend to generate high false positive rates by classifying healthy commits as risky. Finally, they do not provide sufficient insights to developers on how to fix the detected risky commits. In this paper, we propose an approach, called CLEVER (Combining Levels of Bug Prevention and Resolution techniques), which relies on a two-phase process for intercepting risky commits before they reach the central repository. When applied to 12 Ubisoft systems, the results show that CLEVER can detect risky commits with 79{\%} precision and 65{\%} recall, which outperforms the performance of Commit-guru, a recent approach that was proposed in the literature. In addition, CLEVER is able to recommend qualitative fixes to developers on how to fix risky commits in 66.7{\%} of the cases.},
	author = {Nayrolles, Mathieu and Hamou-Lhadj, Abdelwahab},
	doi = {10.1145/3196398.3196438},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/CLEVER - Combining Code Metrics with Clone Detection for Just-In-Time Fault Prevention and Resolution in Large Industrial Projects.pdf:pdf},
	isbn = {9781450357166},
	issn = {02705257},
	journal = {Proceedings - International Conference on Software Engineering},
	keywords = {defect predictions,fault fixing,software evolution,software maintenance},
	mendeley-groups = {Approach,Primary Approaches},
	pages = {153--164},
	publisher = {ACM},
	title = {{CLEVER: Combining code metrics with clone detection for just-in-time fault prevention and resolution in large industrial projects}},
	year = {2018}
}
@article{Kitchenham2004,
	abstract = {The objective of this report is to propose a guideline for systematic reviews appropriate for software engineering researchers, including PhD students. A systematic review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guideline presented in this report was derived from three existing guidelines used by medical researchers. The guideline has been adapted to reflect the specific problems of software engineering research. The guideline covers three phases of a systematic review: planning the review, conducting the review and reporting the review. It is at a relatively high level. It does not consider the impact of question type on the review procedures, nor does it specify in detail mechanisms needed to undertake meta-analysis.},
	author = {Kitchenham, Barbara},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Procedures for Performing Systematic Reviews.pdf:pdf},
	mendeley-groups = {On Studies},
	title = {{Procedures for Performing Systematic Reviews}},
	year = {2004}
}
@article{Kitchenham2009,
	abstract = {Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	author = {Kitchenham, Barbara and {Pearl Brereton}, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
	doi = {10.1016/j.infsof.2008.09.009},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Systematic literature reviews in software engineering â€“ A systematic literatur review.pdf:pdf},
	issn = {09505849},
	journal = {Information and Software Technology},
	keywords = {Cost estimation,Evidence-based software engineering,Systematic literature review,Systematic review quality,Tertiary study},
	mendeley-groups = {On Studies},
	number = {1},
	pages = {7--15},
	publisher = {Elsevier B.V.},
	title = {{Systematic literature reviews in software engineering - A systematic literature review}},
	url = {http://dx.doi.org/10.1016/j.infsof.2008.09.009},
	volume = {51},
	year = {2009}
}
@article{Yang2015,
	abstract = {Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22{\%} more bugs than Kamei et al's approach (51.04{\%} versus 18.82{\%} on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects.},
	author = {Yang, Xinli and Lo, David and Xia, Xin and Zhang, Yun and Sun, Jianling},
	doi = {10.1109/QRS.2015.14},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Deep Learning for Just-In-Time Defect Prediction.pdf:pdf},
	isbn = {9781467379892},
	journal = {Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015},
	keywords = {Cost Effectiveness,Deep Belief Network,Deep Learning,Just-In-Time Defect Prediction},
	mendeley-groups = {Approach,Primary Approaches},
	number = {1},
	pages = {17--26},
	title = {{Deep Learning for Just-in-Time Defect Prediction}},
	year = {2015}
}
@article{Goyal2017,
	abstract = {Transparent environments and social-coding platforms as GitHub help developers to stay abreast of changes during the development and maintenance phase of a project. Especially, notification feeds can help developers to learn about relevant changes in other projects. Unfortunately, transparent environments can quickly overwhelm developers with too many notifications, such that they lose the important ones in a sea of noise. Complementing existing prioritization and filtering strategies based on binary compatibility and code ownership, we develop an anomaly detection mechanism to identify unusual commits in a repository, which stand out with respect to other changes in the same repository or by the same developer. Among others, we detect exceptionally large commits, commits at unusual times, and commits touching rarely changed file types given the characteristics of a particular repository or developer. We automatically flag unusual commits on GitHub through a browser plug-in. In an interactive survey with 173 active GitHub users, rating commits in a project of their interest, we found that, although our unusual score is only a weak predictor of whether developers want to be notified about a commit, information about unusual characteristics of a commit changes how developers regard commits. Our anomaly detection mechanism is a building block for scaling transparent environments.},
	author = {Goyal, Raman and Ferreira, Gabriel and K{\"{a}}stner, Christian and Herbsleb, James},
	doi = {10.1002/smr.1893},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/Identifying unusual commits on GitHub.pdf:pdf},
	issn = {20477481},
	journal = {Journal of Software: Evolution and Process},
	keywords = {Anomaly detection,Information overload,Notification feeds,Software ecosystems,Transparent environments},
	mendeley-groups = {Approach,Primary Approaches},
	number = {August 2017},
	pages = {1--16},
	title = {{Identifying unusual commits on GitHub}},
	year = {2017}
}
@article{Kamei2013,
	abstract = {Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone ({\&} risky{\&}) software changes instead of files or packages. We refer to this type of quality assurance activity as {\&} Just-In-Time Quality Assurance,{\&} because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that {\&} Just-In-Time Quality Assurance{\&} may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software. {\textcopyright} 1976-2012 IEEE.},
	author = {Kamei, Yasutaka and Shihab, Emad and Adams, Bram and Hassan, Ahmed E. and Mockus, Audris and Sinha, Anand and Ubayashi, Naoyasu},
	doi = {10.1109/TSE.2012.70},
	file = {:C$\backslash$:/Users/Lukas Bach/OneDrive/Commit Validation/papers/A Large-Scale Empirical Study of Just-in-Time Quality Assurance.pdf:pdf},
	issn = {00985589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {Maintenance,defect prediction,just-in-time prediction,mining software repositories,software metrics},
	mendeley-groups = {Primary Approaches,Approach},
	number = {6},
	pages = {757--773},
	title = {{A large-scale empirical study of just-in-time quality assurance}},
	volume = {39},
	year = {2013}
}
